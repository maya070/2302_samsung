{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# < Evaluation Metrics >\n",
    "https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Binary Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:       [1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1]\n",
      "Predictions:  [0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "np.random.seed(99)\n",
    "\n",
    "y_true = np.random.randint(2, size=20)             # 20 labels / 10 classes\n",
    "y_pred = np.random.randint(2, size=20)\n",
    "\n",
    "print('Labels:      ', y_true)\n",
    "print('Predictions: ', y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 6]\n",
      " [2 5]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#                                  Predicted Condition\n",
    "#                      ---------------------------------------------\n",
    "#                      |      Positive      |       Negative       |\n",
    "#           --------------------------------------------------------\n",
    "# Actual    | Positive | True Positive (TP)  | False Negative (FN) |\n",
    "#           --------------------------------------------------------\n",
    "# Condition | Negative | False Positive (FP) | True Negative (TN)  |\n",
    "#           --------------------------------------------------------\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred, labels=[1, 0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Accuracy ($\\frac{TP + TN}{TP + FN + FP + TN}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6  or  60.0%\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_true, y_pred), ' or ', str(accuracy_score(y_true, y_pred)*100)+'%')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Precision ($\\frac{TP}{TP + FP}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:  0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "print('Class 1: ', precision_score(y_true, y_pred))\n",
    "# print('Class 0: ', precision_score(y_true, y_pred, pos_label=0))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Recall ($\\frac{TP}{TP + FN}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:  0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "print('Class 1: ', recall_score(y_true, y_pred))\n",
    "# print('Class 0: ', recall_score(y_true, y_pred, pos_label=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - F1-Score ($2 * \\frac{Precision * Recall}{Precision + Recall} = \\frac{TP + TP}{TP + FP + TP + FN} = \\frac{2TP}{2TP + FP + FN}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:  0.6363636363636364\n"
     ]
    }
   ],
   "source": [
    "print('Class 1: ', f1_score(y_true, y_pred))\n",
    "# print('Class 0: ', f1_score(y_true, y_pred, pos_label=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.455     0.714     0.556         7\n",
      "           1      0.778     0.538     0.636        13\n",
      "\n",
      "    accuracy                          0.600        20\n",
      "   macro avg      0.616     0.626     0.596        20\n",
      "weighted avg      0.665     0.600     0.608        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:   [1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1]\n",
      "Model 1:  [0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0]\n",
      "Model 2:  [0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = np.random.randint(2, size=20)\n",
    "\n",
    "print('Labels:  ', y_true)\n",
    "print('Model 1: ', y_pred)\n",
    "print('Model 2: ', y_pred2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Acc.  Prec. Rec.  F1\n",
      "Model 1:  0.60  0.78  0.54  0.64\n",
      "Model 2:  0.55  0.70  0.54  0.61\n"
     ]
    }
   ],
   "source": [
    "print('          Acc.  Prec. Rec.  F1')\n",
    "print('Model 1:  {:.2f}  {:.2f}  {:.2f}  {:.2f}'.format(accuracy_score(y_true, y_pred), precision_score(y_true, y_pred), recall_score(y_true, y_pred), f1_score(y_true, y_pred)))\n",
    "print('Model 2:  {:.2f}  {:.2f}  {:.2f}  {:.2f}'.format(accuracy_score(y_true, y_pred2), precision_score(y_true, y_pred2), recall_score(y_true, y_pred2), f1_score(y_true, y_pred2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.455     0.714     0.556         7\n",
      "           1      0.778     0.538     0.636        13\n",
      "\n",
      "    accuracy                          0.600        20\n",
      "   macro avg      0.616     0.626     0.596        20\n",
      "weighted avg      0.665     0.600     0.608        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true, y_pred, digits=3))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-class Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:       [1 1 0 1 0 2 0 1 0 1 2 1 1 0 1 2 0 2 0 0]\n",
      "Predictions:  [0 0 0 1 2 2 1 2 0 0 1 2 1 1 0 1 1 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(99)\n",
    "\n",
    "y_true = np.random.randint(3, size=20)             # 20 labels / 3 classes\n",
    "y_pred = np.random.randint(3, size=20)\n",
    "\n",
    "print('Labels:      ', y_true)\n",
    "print('Predictions: ', y_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3 3]\n",
      " [4 2 2]\n",
      " [0 2 2]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true, y_pred, labels=[0, 1, 2]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Precision, Recall, F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 0.3333  0.2500  0.2857\n",
      "Class 1: 0.2857  0.2500  0.2667\n",
      "Class 2: 0.2857  0.5000  0.3636\n"
     ]
    }
   ],
   "source": [
    "for_0_true = y_true.copy()\n",
    "for_0_pred = y_pred.copy()\n",
    "\n",
    "for_0_true[for_0_true!=0] = 1\n",
    "for_0_pred[for_0_pred!=0] = 1\n",
    "\n",
    "print('Class 0: {:.4f}  {:.4f}  {:.4f}'.format(precision_score(for_0_true, for_0_pred, pos_label=0), recall_score(for_0_true, for_0_pred, pos_label=0), f1_score(for_0_true, for_0_pred, pos_label=0)))\n",
    "\n",
    "for_1_true = y_true.copy()\n",
    "for_1_pred = y_pred.copy()\n",
    "\n",
    "for_1_true[for_1_true!=1] = 0\n",
    "for_1_pred[for_1_pred!=1] = 0\n",
    "\n",
    "print('Class 1: {:.4f}  {:.4f}  {:.4f}'.format(precision_score(for_1_true, for_1_pred, pos_label=1), recall_score(for_1_true, for_1_pred, pos_label=1), f1_score(for_1_true, for_1_pred, pos_label=1)))\n",
    "\n",
    "for_2_true = y_true.copy()\n",
    "for_2_pred = y_pred.copy()\n",
    "\n",
    "for_2_true[for_2_true!=2] = 0\n",
    "for_2_pred[for_2_pred!=2] = 0\n",
    "for_2_true[for_2_true==2] = 1\n",
    "for_2_pred[for_2_pred==2] = 1\n",
    "\n",
    "print('Class 2: {:.4f}  {:.4f}  {:.4f}'.format(precision_score(for_2_true, for_2_pred, pos_label=1), recall_score(for_2_true, for_2_pred, pos_label=1), f1_score(for_2_true, for_2_pred, pos_label=1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurayc: 0.30\n"
     ]
    }
   ],
   "source": [
    "print('Accurayc: {:.2f}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Macro/Micro-Precision, Recall, F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Prec. Rec.  F1\n",
      "Micro: 0.3000  0.3000  0.3000\n",
      "Macro: 0.3016  0.3333  0.3053\n"
     ]
    }
   ],
   "source": [
    "print('       Prec. Rec.  F1')\n",
    "print('Micro: {:.4f}  {:.4f}  {:.4f}'.format(precision_score(y_true, y_pred, average='micro'), recall_score(y_true, y_pred, average='micro'), f1_score(y_true, y_pred, average='micro')))\n",
    "print('Macro: {:.4f}  {:.4f}  {:.4f}'.format(precision_score(y_true, y_pred, average='macro'), recall_score(y_true, y_pred, average='macro'), f1_score(y_true, y_pred, average='macro')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.333     0.250     0.286         8\n",
      "           1      0.286     0.250     0.267         8\n",
      "           2      0.286     0.500     0.364         4\n",
      "\n",
      "    accuracy                          0.300        20\n",
      "   macro avg      0.302     0.333     0.305        20\n",
      "weighted avg      0.305     0.300     0.294        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true, y_pred, digits=3))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-label Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels\t\tPredictions\n",
      "[1 1 1] \t [1 1 1]\n",
      "[0 1 0] \t [1 0 1]\n",
      "[0 0 1] \t [1 0 1]\n",
      "[0 1 1] \t [0 0 0]\n",
      "[0 1 1] \t [1 0 1]\n",
      "[1 1 0] \t [1 0 1]\n",
      "[1 1 0] \t [1 1 1]\n",
      "[1 0 1] \t [1 0 0]\n",
      "[0 0 0] \t [0 1 1]\n",
      "[0 0 0] \t [0 0 0]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "np.random.seed(99)\n",
    "\n",
    "y_true = np.random.randint(2, size=3*10).reshape([10, 3])             # 10 samples / 3 classes\n",
    "y_pred = np.random.randint(2, size=3*10).reshape([10, 3])\n",
    "\n",
    "\n",
    "print('Labels\\t\\tPredictions')\n",
    "for true, pred in zip(y_true, y_pred):\n",
    "    print(true, '\\t', pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3, 3],\n",
       "        [0, 4]],\n",
       "\n",
       "       [[3, 1],\n",
       "        [4, 2]],\n",
       "\n",
       "       [[1, 4],\n",
       "        [2, 3]]], dtype=int64)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(y_true, y_pred, labels=[0, 1, 2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Precision, Recall, F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Prec.   Rec.    F1\n",
      "Class 0: 0.5714  1.0000  0.7273\n",
      "Class 1: 0.6667  0.3333  0.4444\n",
      "Class 2: 0.4286  0.6000  0.5000\n"
     ]
    }
   ],
   "source": [
    "print('         Prec.   Rec.    F1')\n",
    "print('Class 0: {:.4f}  {:.4f}  {:.4f}'.format(precision_score(y_true[:,0], y_pred[:,0]), recall_score(y_true[:,0], y_pred[:,0]), f1_score(y_true[:,0], y_pred[:,0])))\n",
    "print('Class 1: {:.4f}  {:.4f}  {:.4f}'.format(precision_score(y_true[:,1], y_pred[:,1]), recall_score(y_true[:,1], y_pred[:,1]), f1_score(y_true[:,1], y_pred[:,1])))\n",
    "print('Class 2: {:.4f}  {:.4f}  {:.4f}'.format(precision_score(y_true[:,2], y_pred[:,2]), recall_score(y_true[:,2], y_pred[:,2]), f1_score(y_true[:,2], y_pred[:,2])))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5714    1.0000    0.7273         4\n",
      "           1     0.6667    0.3333    0.4444         6\n",
      "           2     0.4286    0.6000    0.5000         5\n",
      "\n",
      "   micro avg     0.5294    0.6000    0.5625        15\n",
      "   macro avg     0.5556    0.6444    0.5572        15\n",
      "weighted avg     0.5619    0.6000    0.5384        15\n",
      " samples avg     0.4167    0.4500    0.4133        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true, y_pred, digits=4, zero_division=0))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "4_3_CNN_VGG_ResNet_Cifar10_Practice.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d97a0948663b0b2eced954f294a163a1ac62d5a5cf016fffcde35ae285e7c733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
