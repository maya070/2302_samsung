{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils import data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS     = 40\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "                       transforms.RandomHorizontalFlip(),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5,), (0.5,))\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.MNIST(\n",
    "    root      = '../data/', \n",
    "    train     = True,\n",
    "    download  = True,\n",
    "    transform = transform\n",
    ")\n",
    "testset = datasets.MNIST(\n",
    "    root      = '../data/', \n",
    "    train     = False,\n",
    "    download  = True,\n",
    "    transform = transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(\n",
    "    dataset     = trainset,\n",
    "    batch_size  = BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_loader = data.DataLoader(\n",
    "    dataset     = testset,\n",
    "    batch_size  = BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 1, 1, 5, 4, 0, 3, 6, 1, 6, 5, 6, 3, 4, 6, 4, 7, 0, 2, 8, 0, 9, 9, 6,\n",
      "        8, 0, 5, 4, 5, 9, 9, 0, 9, 1, 2, 6, 1, 7, 8, 1, 8, 6, 5, 4, 1, 9, 7, 6,\n",
      "        0, 1, 6, 7, 6, 1, 6, 9, 2, 0, 3, 7, 8, 3, 7, 4])\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "six\n",
      "one\n",
      "one\n",
      "five\n",
      "four\n",
      "zero\n",
      "three\n",
      "six\n",
      "one\n",
      "six\n",
      "five\n",
      "six\n",
      "three\n",
      "four\n",
      "six\n",
      "four\n",
      "seven\n",
      "zero\n",
      "two\n",
      "eight\n",
      "zero\n",
      "nine\n",
      "nine\n",
      "six\n",
      "eight\n",
      "zero\n",
      "five\n",
      "four\n",
      "five\n",
      "nine\n",
      "nine\n",
      "zero\n",
      "nine\n",
      "one\n",
      "two\n",
      "six\n",
      "one\n",
      "seven\n",
      "eight\n",
      "one\n",
      "eight\n",
      "six\n",
      "five\n",
      "four\n",
      "one\n",
      "nine\n",
      "seven\n",
      "six\n",
      "zero\n",
      "one\n",
      "six\n",
      "seven\n",
      "six\n",
      "one\n",
      "six\n",
      "nine\n",
      "two\n",
      "zero\n",
      "three\n",
      "seven\n",
      "eight\n",
      "three\n",
      "seven\n",
      "four\n"
     ]
    }
   ],
   "source": [
    "CLASSES = {\n",
    "    0: 'zero',\n",
    "    1: 'one',\n",
    "    2: 'two',\n",
    "    3: 'three',\n",
    "    4: 'four',\n",
    "    5: 'five',\n",
    "    6: 'six',\n",
    "    7: 'seven',\n",
    "    8: 'eight',\n",
    "    9: 'nine'\n",
    "}\n",
    "\n",
    "\n",
    "for label in labels:\n",
    "    index = label.item()\n",
    "    print(CLASSES[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "\n",
    "item_img = images[idx]\n",
    "item_npimg = item_img.squeeze().numpy()\n",
    "plt.title(CLASSES[labels[idx].item()])\n",
    "print(item_npimg.shape)\n",
    "plt.imshow(item_npimg, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  \n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = Net().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target,\n",
    "                                         reduction='sum').item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.323732\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.311864\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.272814\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.269629\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.260193\n",
      "[1] Test Loss: 2.2467, Accuracy: 32.41%\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.261455\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.232894\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.148907\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.034325\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.862490\n",
      "[2] Test Loss: 1.7541, Accuracy: 56.66%\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.801105\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.555452\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.526296\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.255500\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.244716\n",
      "[3] Test Loss: 1.0972, Accuracy: 68.34%\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.054251\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.053030\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.137899\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.903471\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.097534\n",
      "[4] Test Loss: 0.8502, Accuracy: 73.79%\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.870562\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.828092\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.741687\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.802395\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.715464\n",
      "[5] Test Loss: 0.7362, Accuracy: 76.72%\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.728907\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.726325\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.669869\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.720801\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.680491\n",
      "[6] Test Loss: 0.6704, Accuracy: 78.52%\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.873560\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.783721\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.767666\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.731977\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.532270\n",
      "[7] Test Loss: 0.6259, Accuracy: 80.10%\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.639074\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.584815\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.543321\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.644632\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.434011\n",
      "[8] Test Loss: 0.5959, Accuracy: 80.62%\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.602555\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.573936\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.708729\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.637048\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.425368\n",
      "[9] Test Loss: 0.5674, Accuracy: 81.88%\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.675427\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.416949\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.338234\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.586418\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.523194\n",
      "[10] Test Loss: 0.5504, Accuracy: 82.50%\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.912765\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.480590\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.619421\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.585126\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.542211\n",
      "[11] Test Loss: 0.5330, Accuracy: 82.67%\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.800282\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.458045\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.682959\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.603446\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.309354\n",
      "[12] Test Loss: 0.5158, Accuracy: 83.32%\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.436457\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.629186\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.751895\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.797777\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.308820\n",
      "[13] Test Loss: 0.4988, Accuracy: 83.86%\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.561620\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.495395\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.383911\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.758781\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.372925\n",
      "[14] Test Loss: 0.4884, Accuracy: 84.24%\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.709628\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.560667\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.471812\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.498453\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.584763\n",
      "[15] Test Loss: 0.4706, Accuracy: 84.81%\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.434333\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.514581\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.503962\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.324424\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.358791\n",
      "[16] Test Loss: 0.4548, Accuracy: 85.22%\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.399081\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.686157\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.577609\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.312719\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.421413\n",
      "[17] Test Loss: 0.4458, Accuracy: 85.80%\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.697016\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.527341\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.489231\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.299626\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.346957\n",
      "[18] Test Loss: 0.4303, Accuracy: 86.18%\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.324074\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.545187\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.442608\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.406727\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.398566\n",
      "[19] Test Loss: 0.4235, Accuracy: 86.40%\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.443999\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.500453\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.637397\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.331340\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.409619\n",
      "[20] Test Loss: 0.4096, Accuracy: 86.96%\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.433320\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 0.372210\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.423990\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 0.316995\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.424712\n",
      "[21] Test Loss: 0.3974, Accuracy: 87.21%\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.651763\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 0.494821\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.235234\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 0.592624\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.265334\n",
      "[22] Test Loss: 0.3930, Accuracy: 87.48%\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.516954\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 0.313994\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.498694\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 0.451727\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.231930\n",
      "[23] Test Loss: 0.3795, Accuracy: 88.01%\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.382107\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 0.460715\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.589220\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 0.289890\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.457415\n",
      "[24] Test Loss: 0.3721, Accuracy: 88.23%\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.287419\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 0.289949\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.322669\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.356281\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.353179\n",
      "[25] Test Loss: 0.3656, Accuracy: 88.61%\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.320752\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 0.274451\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.595272\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 0.329667\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.351240\n",
      "[26] Test Loss: 0.3552, Accuracy: 88.77%\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.511596\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 0.434859\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.169365\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 0.420618\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.325648\n",
      "[27] Test Loss: 0.3495, Accuracy: 88.90%\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.246905\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 0.449159\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.410477\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 0.209701\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.346072\n",
      "[28] Test Loss: 0.3411, Accuracy: 89.15%\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.187274\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 0.166215\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.234664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 0.269801\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.216933\n",
      "[29] Test Loss: 0.3359, Accuracy: 89.29%\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.392475\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 0.371376\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.263359\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 0.244516\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.262344\n",
      "[30] Test Loss: 0.3275, Accuracy: 89.76%\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.325312\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 0.342637\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.216260\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 0.306035\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.285057\n",
      "[31] Test Loss: 0.3224, Accuracy: 89.96%\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.421398\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 0.416357\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.275496\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 0.336517\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.429863\n",
      "[32] Test Loss: 0.3156, Accuracy: 89.96%\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.319225\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 0.315990\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.410873\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 0.150671\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.239820\n",
      "[33] Test Loss: 0.3099, Accuracy: 90.42%\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.279317\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 0.507767\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.423873\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 0.354779\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.480044\n",
      "[34] Test Loss: 0.3035, Accuracy: 90.54%\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.381853\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 0.342527\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.233260\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 0.200696\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.430886\n",
      "[35] Test Loss: 0.2990, Accuracy: 90.99%\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.296410\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 0.264649\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.470391\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 0.291368\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.198366\n",
      "[36] Test Loss: 0.2926, Accuracy: 90.96%\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.534715\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 0.226209\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.234866\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 0.161712\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.315570\n",
      "[37] Test Loss: 0.2857, Accuracy: 91.26%\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.184085\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 0.319251\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.199112\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 0.331018\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.351098\n",
      "[38] Test Loss: 0.2851, Accuracy: 91.13%\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.425916\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 0.252609\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.316007\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 0.231076\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.240461\n",
      "[39] Test Loss: 0.2776, Accuracy: 91.45%\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.164552\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 0.213314\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.373317\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 0.343891\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.256331\n",
      "[40] Test Loss: 0.2708, Accuracy: 91.67%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "          epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = 6\n",
    "rows = 6\n",
    "fig = plt.figure(figsize=(10,10))\n",
    " \n",
    "model.eval()\n",
    "for i in range(1, columns*rows+1):\n",
    "    data_idx = np.random.randint(len(testset))\n",
    "    input_img = testset[data_idx][0].unsqueeze(dim=0).to(DEVICE) \n",
    " \n",
    "    output = model(input_img)\n",
    "    _, argmax = torch.max(output, 1)\n",
    "    pred = CLASSES[argmax.item()]\n",
    "    label = CLASSES[testset[data_idx][1]]\n",
    "    \n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    if pred == label:\n",
    "        plt.title(pred + ', right')\n",
    "        cmap = 'Blues'\n",
    "    else:\n",
    "        plt.title('N ' + pred + ' B ' +  label)\n",
    "        cmap = 'Reds'\n",
    "    plot_img = testset[data_idx][0][0,:,:]\n",
    "    plt.imshow(plot_img, cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
