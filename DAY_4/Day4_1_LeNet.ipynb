{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0VDw5asHQkFv"
   },
   "source": [
    "# < LeNet-5 >"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LeNet_Original_Image_48T74Lc.jpg](https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets, utils\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Set GPU Envrionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Set Hyper-parameters\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Training\n",
    "EPOCHS     = 10\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# For Optimizer\n",
    "LR         = 0.001\n",
    "MOMENTUM   = 0.9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Prepare the Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Calculate Mean and Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "(50000, 3, 32, 32)\n",
      "0.49139968 0.48215827 0.44653124\n",
      "0.24703233 0.24348505 0.26158768\n",
      "0.20220213 0.19931543 0.20086348\n"
     ]
    }
   ],
   "source": [
    "train_CIFAR10_raw = datasets.CIFAR10(root='./dataset/CIFAR10', train=True, download=True, transform=transforms.ToTensor())\n",
    "imgs = [item[0] for item in train_CIFAR10_raw] # item[0] and item[1] are image and its label\n",
    "imgs = torch.stack(imgs, dim=0).numpy()\n",
    "print(imgs.shape)\n",
    "\n",
    "# calculate mean over each channel (r,g,b)\n",
    "mean_r = imgs[:,0].mean()\n",
    "mean_g = imgs[:,1].mean()\n",
    "mean_b = imgs[:,2].mean()\n",
    "print(mean_r,mean_g,mean_b)\n",
    "\n",
    "# calculate std over each channel (r,g,b)\n",
    "std_r = imgs[:,0].std()\n",
    "std_g = imgs[:,1].std()\n",
    "std_b = imgs[:,2].std()\n",
    "print(std_r,std_g,std_b)\n",
    "\n",
    "# calculate mean_std over each channel std (r,g,b)\n",
    "std_all = np.array([np.std(x, axis=(1,2)) for x in imgs])\n",
    "std_r = std_all[:,0].mean()\n",
    "std_g = std_all[:,1].mean()\n",
    "std_b = std_all[:,2].mean()\n",
    "print(std_r,std_g,std_b)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Compare a Image Before/After Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Before Normalization:  [0.23137255 0.16862746 0.19607843 0.26666668 0.38431373]\n",
      "After Normalization:  [-1.2853558 -1.5955144 -1.45982   -1.1108913 -0.5293439]\n"
     ]
    }
   ],
   "source": [
    "transform_CIFAR10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_CIFAR10_normalized = datasets.CIFAR10(root='./dataset/CIFAR10', train=True, download=True, transform=transform_CIFAR10)\n",
    "\n",
    "print('Before Normalization: ', train_CIFAR10_raw[0][0].detach().numpy()[0,0,:5])\n",
    "print('After Normalization: ', train_CIFAR10_normalized[0][0].detach().numpy()[0,0,:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - DataLoader for Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        root      = './dataset/CIFAR10', \n",
    "        train     = True,\n",
    "        download  = True,\n",
    "        transform = transform_CIFAR10),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory = True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        root      = './dataset/CIFAR10', \n",
    "        train     = False,\n",
    "        download  = True,\n",
    "        transform = transform_CIFAR10),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory = True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Make the Model (LeNet-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LeNet_Original_Image_48T74Lc.jpg](https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All the layer have tanh activaiton.\n",
    "- C1 and C2 are 5 X 5 convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ji6VxuocVf80"
   },
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        ###############################\n",
    "        #                             #\n",
    "        #  Please, complete the code  #\n",
    "        #                             #\n",
    "        ###############################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ###############################\n",
    "        #                             #\n",
    "        #  Please, complete the code  #\n",
    "        #                             #\n",
    "        ###############################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 28, 28]             456\n",
      "         AvgPool2d-2            [-1, 6, 14, 14]               0\n",
      "            Conv2d-3           [-1, 16, 10, 10]           2,416\n",
      "         AvgPool2d-4             [-1, 16, 5, 5]               0\n",
      "            Conv2d-5            [-1, 120, 1, 1]          48,120\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 62,006\n",
      "Trainable params: 62,006\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.31\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NullA\\realanaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "model     = LeNet5().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "            test_loss += F.cross_entropy(output, target,\n",
    "                                         reduction='sum').item()\n",
    "\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.306708\n",
      "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 2.297889\n",
      "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 2.298924\n",
      "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 2.297415\n",
      "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 2.284962\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 2.292728\n",
      "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 2.273861\n",
      "Train Epoch: 1 [35840/50000 (71%)]\tLoss: 2.274694\n",
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 2.269002\n",
      "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 2.257367\n",
      "[1] Test Loss: 2.2510, Accuracy: 21.27%\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 2.248024\n",
      "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 2.232392\n",
      "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 2.211924\n",
      "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 2.203253\n",
      "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 2.191319\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 2.175657\n",
      "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 2.170929\n",
      "Train Epoch: 2 [35840/50000 (71%)]\tLoss: 2.172102\n",
      "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 2.101022\n",
      "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 2.030091\n",
      "[2] Test Loss: 2.0892, Accuracy: 23.79%\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 2.093430\n",
      "Train Epoch: 3 [5120/50000 (10%)]\tLoss: 2.082292\n",
      "Train Epoch: 3 [10240/50000 (20%)]\tLoss: 2.055154\n",
      "Train Epoch: 3 [15360/50000 (31%)]\tLoss: 2.017463\n",
      "Train Epoch: 3 [20480/50000 (41%)]\tLoss: 2.042628\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 2.003118\n",
      "Train Epoch: 3 [30720/50000 (61%)]\tLoss: 2.088115\n",
      "Train Epoch: 3 [35840/50000 (71%)]\tLoss: 2.029449\n",
      "Train Epoch: 3 [40960/50000 (82%)]\tLoss: 2.039073\n",
      "Train Epoch: 3 [46080/50000 (92%)]\tLoss: 2.081873\n",
      "[3] Test Loss: 2.0184, Accuracy: 25.67%\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 2.041915\n",
      "Train Epoch: 4 [5120/50000 (10%)]\tLoss: 1.961948\n",
      "Train Epoch: 4 [10240/50000 (20%)]\tLoss: 2.020801\n",
      "Train Epoch: 4 [15360/50000 (31%)]\tLoss: 2.012743\n",
      "Train Epoch: 4 [20480/50000 (41%)]\tLoss: 1.980973\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.997204\n",
      "Train Epoch: 4 [30720/50000 (61%)]\tLoss: 1.986309\n",
      "Train Epoch: 4 [35840/50000 (71%)]\tLoss: 2.000415\n",
      "Train Epoch: 4 [40960/50000 (82%)]\tLoss: 2.050468\n",
      "Train Epoch: 4 [46080/50000 (92%)]\tLoss: 1.935176\n",
      "[4] Test Loss: 1.9812, Accuracy: 27.90%\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 2.005034\n",
      "Train Epoch: 5 [5120/50000 (10%)]\tLoss: 2.011503\n",
      "Train Epoch: 5 [10240/50000 (20%)]\tLoss: 2.042828\n",
      "Train Epoch: 5 [15360/50000 (31%)]\tLoss: 1.948594\n",
      "Train Epoch: 5 [20480/50000 (41%)]\tLoss: 2.047333\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.936974\n",
      "Train Epoch: 5 [30720/50000 (61%)]\tLoss: 1.936265\n",
      "Train Epoch: 5 [35840/50000 (71%)]\tLoss: 1.920604\n",
      "Train Epoch: 5 [40960/50000 (82%)]\tLoss: 1.974476\n",
      "Train Epoch: 5 [46080/50000 (92%)]\tLoss: 2.014022\n",
      "[5] Test Loss: 1.9496, Accuracy: 29.41%\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.993867\n",
      "Train Epoch: 6 [5120/50000 (10%)]\tLoss: 1.891857\n",
      "Train Epoch: 6 [10240/50000 (20%)]\tLoss: 1.901289\n",
      "Train Epoch: 6 [15360/50000 (31%)]\tLoss: 2.021972\n",
      "Train Epoch: 6 [20480/50000 (41%)]\tLoss: 1.926580\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 2.003166\n",
      "Train Epoch: 6 [30720/50000 (61%)]\tLoss: 1.981581\n",
      "Train Epoch: 6 [35840/50000 (71%)]\tLoss: 1.865369\n",
      "Train Epoch: 6 [40960/50000 (82%)]\tLoss: 1.925402\n",
      "Train Epoch: 6 [46080/50000 (92%)]\tLoss: 2.024465\n",
      "[6] Test Loss: 1.9217, Accuracy: 31.07%\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.869472\n",
      "Train Epoch: 7 [5120/50000 (10%)]\tLoss: 1.916200\n",
      "Train Epoch: 7 [10240/50000 (20%)]\tLoss: 1.962779\n",
      "Train Epoch: 7 [15360/50000 (31%)]\tLoss: 1.927070\n",
      "Train Epoch: 7 [20480/50000 (41%)]\tLoss: 1.952946\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 1.878448\n",
      "Train Epoch: 7 [30720/50000 (61%)]\tLoss: 1.915883\n",
      "Train Epoch: 7 [35840/50000 (71%)]\tLoss: 1.907145\n",
      "Train Epoch: 7 [40960/50000 (82%)]\tLoss: 1.920130\n",
      "Train Epoch: 7 [46080/50000 (92%)]\tLoss: 1.912717\n",
      "[7] Test Loss: 1.8912, Accuracy: 32.00%\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.891770\n",
      "Train Epoch: 8 [5120/50000 (10%)]\tLoss: 1.844362\n",
      "Train Epoch: 8 [10240/50000 (20%)]\tLoss: 1.912493\n",
      "Train Epoch: 8 [15360/50000 (31%)]\tLoss: 1.859105\n",
      "Train Epoch: 8 [20480/50000 (41%)]\tLoss: 1.827079\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 1.810830\n",
      "Train Epoch: 8 [30720/50000 (61%)]\tLoss: 1.918383\n",
      "Train Epoch: 8 [35840/50000 (71%)]\tLoss: 1.880162\n",
      "Train Epoch: 8 [40960/50000 (82%)]\tLoss: 1.863377\n",
      "Train Epoch: 8 [46080/50000 (92%)]\tLoss: 1.824624\n",
      "[8] Test Loss: 1.8616, Accuracy: 33.97%\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.816936\n",
      "Train Epoch: 9 [5120/50000 (10%)]\tLoss: 1.823163\n",
      "Train Epoch: 9 [10240/50000 (20%)]\tLoss: 1.857411\n",
      "Train Epoch: 9 [15360/50000 (31%)]\tLoss: 1.865177\n",
      "Train Epoch: 9 [20480/50000 (41%)]\tLoss: 1.807258\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 1.859970\n",
      "Train Epoch: 9 [30720/50000 (61%)]\tLoss: 1.776032\n",
      "Train Epoch: 9 [35840/50000 (71%)]\tLoss: 1.813314\n",
      "Train Epoch: 9 [40960/50000 (82%)]\tLoss: 1.826445\n",
      "Train Epoch: 9 [46080/50000 (92%)]\tLoss: 1.806786\n",
      "[9] Test Loss: 1.8336, Accuracy: 34.67%\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.830749\n",
      "Train Epoch: 10 [5120/50000 (10%)]\tLoss: 1.832398\n",
      "Train Epoch: 10 [10240/50000 (20%)]\tLoss: 1.855039\n",
      "Train Epoch: 10 [15360/50000 (31%)]\tLoss: 1.810975\n",
      "Train Epoch: 10 [20480/50000 (41%)]\tLoss: 1.804808\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 1.917251\n",
      "Train Epoch: 10 [30720/50000 (61%)]\tLoss: 1.809358\n",
      "Train Epoch: 10 [35840/50000 (71%)]\tLoss: 1.894740\n",
      "Train Epoch: 10 [40960/50000 (82%)]\tLoss: 1.879936\n",
      "Train Epoch: 10 [46080/50000 (92%)]\tLoss: 1.774947\n",
      "[10] Test Loss: 1.8082, Accuracy: 35.09%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "          epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hmm..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.C1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.S2 = nn.AvgPool2d(2, stride=2)\n",
    "        self.C3 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.S4 = nn.AvgPool2d(2, stride=2)\n",
    "        self.C5 = nn.Conv2d(16, 120, kernel_size=5)\n",
    "        self.F6 = nn.Linear(120, 84)\n",
    "        self.OUTPUT = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.C1(x))\n",
    "        x = F.tanh(self.S2(x))\n",
    "        x = F.tanh(self.C3(x))\n",
    "        x = F.tanh(self.S4(x))\n",
    "        x = F.tanh(self.C5(x))\n",
    "        x = x.view(-1, 120)\n",
    "        x = F.tanh(self.F6(x))\n",
    "        x = self.OUTPUT(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "4_3_CNN_VGG_ResNet_Cifar10_Practice.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d97a0948663b0b2eced954f294a163a1ac62d5a5cf016fffcde35ae285e7c733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
